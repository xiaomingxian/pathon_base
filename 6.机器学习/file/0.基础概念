特征工程

jieba分词器

重要性(评估词语的重要性)：tf*idf  #解决多篇文章中共性但是决定性不高的词语的问题(eg:所以，我们，明天之类的词语)
    #数值约小重要性越低 tf词的频率越高越重要 idf(总文档数量固定/词出现的文档数量[越大log的值越小，对重要性起削弱作用(解决共性词语的非重要影响力)])
    tf:Term frequency:词的频率
    idf:inverse document frequency:逆文档频率[类似与es的倒排索引?]
        公式：log(总文档数量/该词出现的文档数量) x=log2 N  x是以2为底N的对数 ==>2^x=N N越大x越大

归一化：特点：通过对原始数据进行变换把数据映射到(默认0-1)之间
    目的：使一个特征对结果不会造成更大的影响[把得分压缩在一个范围之内?]
    异常点：不合群的数据 极大/极小
    缺点：稳定性不好(鲁棒性)[异常点的影响大]
    公式：作用于每一列 ===> x'=(x-min(x))/(max(x)-min(x))值在0-1
         x''=x'*(mx-mi)+mi  #x''为最终结果

标准化：某一个值对整体的影响不会很大
    优点：稳定性好
    公式：x'=(x-mean)/a  #mean为平均值，a为方差  #作用于某一列
    方差：fc=(x1=mean)^2+(x2-mean)^2+.../n #n为每个特征的样本数 #方差考量数据的稳定性   #作用于某一列
    标准差：a=fc开方
    API：处理后每列来说数据都聚集在均值0附近标准差为1  [默认的基准?]