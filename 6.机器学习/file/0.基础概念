特征工程

jieba分词器

========================================================================

重要性(评估词语的重要性)：tf*idf  #解决多篇文章中共性但是决定性不高的词语的问题(eg:所以，我们，明天之类的词语)
    #数值约小重要性越低 tf词的频率越高越重要 idf(总文档数量固定/词出现的文档数量[越大log的值越小，对重要性起削弱作用(解决共性词语的非重要影响力)])
    tf:Term frequency:词的频率
    idf:inverse document frequency:逆文档频率[类似与es的倒排索引?]
        公式：log(总文档数量/该词出现的文档数量) x=log2 N  x是以2为底N的对数 ==>2^x=N N越大x越大

归一化：特点：通过对原始数据进行变换把数据映射到(默认0-1)之间
    目的：使一个特征对结果不会造成更大的影响[把得分压缩在一个范围之内?]
    异常点：不合群的数据 极大/极小
    缺点：稳定性不好(鲁棒性)[异常点的影响大]
    公式：作用于每一列 ===> x'=(x-min(x))/(max(x)-min(x))值在0-1
         x''=x'*(mx-mi)+mi  #x''为最终结果

标准化：某一个值对整体的影响不会很大
    场景：样本足够多的情况下比较稳定，适合现代嘈杂的大数据场景
    优点：稳定性好
    公式：x'=(x-mean)/a  #mean为平均值，a为方差  #作用于某一列
    方差：fc=(x1=mean)^2+(x2-mean)^2+.../n #n为每个特征的样本数 #方差考量数据的稳定性   #作用于某一列
    标准差：a=fc开方
    API：处理后每列来说数据都聚集在均值0附近标准差为1  [默认的基准?]

sklearn的缺失值处理：
    删除[缺失值达到一定数量建议放弃整行或整列]
    插补[可以通过缺失值的每行或每列的平均数，中位数来填充]

========================================================================

特征预处理
    通过特定的统计方法(数学方法)将数据[数值型数据]转换成算法要求的数据
    数值型数据：标准缩放：
        归一化，标准化，缺失值
    类别型数据：one-hot编码
    时间型：时间的切分

特征选择：
    从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值，也可不改变。
        但是选择后的数据维度肯定比选择前的小，因为只选择了一部分特征
    主要方法(三大武器)：
        Filter(过滤器)：Variance[方差(数据与平均值的距离)] Threshold    #自定义标准
        Embedded(嵌入式)：正则化，决策树
        Wrapper(包裹式)[少用]

=====================================================机器学习中的算法分类
有监督算法[有特征值也有目标值]
无监督算法[有特征值没有目标值]
半监督算法

有监督算法:标值是离散[分类算法(分数标记分类)]的还是连续的[回归算法(打分)]
无监督算法:聚类

训练集：测试集  ==> 70%:30% 80%:20%  75%:25%