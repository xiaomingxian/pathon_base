算法是核心，数据和计算[电脑的计算能力]是基础

#超参数(算法实例化时的参数) 对概率会有影响

数据类型：
    离散型：记录不同类别个体的数目所得的数据[又称计数数据]，全都是整数，且不能细分，也不能进一步提高精度
    连续型：变量可以在任意范围内任取一数，即变量的取值是可以连续的 eg:时间,长度等非整数 可以在区间内任意取值
    区别：离散是区间内不可分，连续是区间内可分

机器学习算法分类：
    监督学习(预测-->特征值+目标值)
        分类：k-邻近算法，贝叶斯分类,决策树与决策森林,逻辑回归,神经网络
        回归：线性回归，岭回归
        标注：隐马尔可夫模型(用得少，已经有成熟的做法，此处略)
    非监督学习(只有特征值)
        聚类：k-means

机器学习开发流程：
    0 原始数据：自有/合作/购买
    1 明确问题：根据数据类型[离散/连续]划分应用种类[分类/回归]
    2 数据的基本处理：pandas处理
    3 特征工程：特征处理
    4 算法预测[根据模型(算法+数据 eg:fit(data)后建立的标准-[我的理解])]
    5 模型评估，判定效果[准确率等]
    上线使用 API 形式提供

====================================== 算法
k-近邻[k个距离]：需要做标准化
    优点：简单，无需估计参数[算法内部的参数]，无需训练[一次就得出结果，不需要多次迭代]
    缺点：
        k的取值：
            很大：容易受k值数量波动 #k=分母(分子造成比例的影响波动)
            很小：容易受异常点影响
        性能低
    使用场景：几乎不用

朴素贝叶斯：概率[朴素：特征独立]   适合文本分类eg:垃圾邮件过滤
    联合概率：P(AB)=P(A)*P(B)
    条件概率：P(A|B)=P(A1|B)*P(A2|B) #B为条件 #A1，A2 条件相互独立互不影响
    贝叶斯公式：P(C|F1,F2...)=P(W|C)*P(C)/P(W)=P(F1,F2...|C)*P(C)/P(F1,F2...)
        说明：
            P(C)：每个文档的概率(某文档类别数/总文档数量)
            P(W|C):给定文档下特征(被预测文档中出现的词)的概率
            计算方式：P(F1|C)=Ni/N
                Ni为该F1词在C类别所有文档中出现的次数
                N为所属类下的文档所有词出现的次数和
        W为给定文档的特征值，C为文档类别
        eg:P(科技|词1,词2...)=P(f1,f2...|科技)/P(W)

        拉普拉斯平滑系数：解决贝叶斯公式中个别数据为0导致计算结果为0的不合理现象===>某个文章是某种类型的概率为0
            P(F1|C)=Ni+a/N+am  #a为指定的系数一般为1,m为训练文档中统计的特征词个数
    优缺点见图
        基于训练集进行预测  词语中的词比较多会对结果进行干扰[其他非主题词语??]



决策树：
    球队例子二分法猜测：比特：信息单位
        32支球队：log32=5比特   ==> 5=-(1/32log1/32+1/32log1/32+.....)
        64支球队：log64=6比特
        当得知一些历史消息猜测的代价会小一些(信息墒)  得知了一些球队的获胜比例 5>-(1/32log1/6+1/32log1/8+.....)
    划分依据：信息增益：当得知一个特征条件后，减少的信息墒的大小--->不确定性的较少数量
    常见决策树使用的算法：ID3:信息增益，最大准则；C4.5：信息增益比，最大准则；
        CART:1.回归树:平方误差 最小 2.分类树:基尼系数 最小准则(粒度更细,sklearn中可以选择划分的默认原则)









